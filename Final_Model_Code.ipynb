{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcs9ia7A_O1f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from skimage import io as skio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "import cv2 as cv\n",
        "import os\n",
        "import shutil\n",
        "import base64\n",
        "import json\n",
        "import skimage.io as skio\n",
        "import io\n",
        "import torchvision.models as models\n",
        "import time\n",
        "from sklearn.metrics import roc_curve, auc, f1_score, precision_score, recall_score\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrJCgGT8OFUy"
      },
      "outputs": [],
      "source": [
        "#File path to the weights file\n",
        "checkpoint_path = \"/home/group18/Final/Final_Model/Final_Weights.pth.tar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfI0olQgwhEz"
      },
      "outputs": [],
      "source": [
        "#Creates Dataset form MIMIC images\n",
        "class MIMIC_Dataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #Retrieves image name from mimic dataset csv\n",
        "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
        "        image = skio.imread(img_path + \".jpg\")\n",
        "        RGB = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "        new_image = Image.fromarray(RGB)\n",
        "        new_image = new_image.resize((256, 256))\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # Atelectasis\n",
        "        results.append(int(self.annotations.iloc[index, 12] == 1.0))\n",
        "\n",
        "        # Cardiomegaly\n",
        "        results.append(int(self.annotations.iloc[index, 13] == 1.0))\n",
        "\n",
        "        # Consolidation\n",
        "        results.append(int(self.annotations.iloc[index, 14] == 1.0))\n",
        "\n",
        "        # Edema\n",
        "        results.append(int(self.annotations.iloc[index, 15] == 1.0))\n",
        "\n",
        "        # No Finding\n",
        "        results.append(int(self.annotations.iloc[index, 20] == 1.0))\n",
        "\n",
        "        # Pleural Effusion\n",
        "        results.append(int(self.annotations.iloc[index, 21] == 1.0))\n",
        "\n",
        "        y_label = torch.tensor(results, dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            new_image = self.transform(new_image)\n",
        "\n",
        "        return new_image, y_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIzsNaTHouPs"
      },
      "outputs": [],
      "source": [
        "#Overalys mask on image used for Grad Cam\n",
        "def show_cam_on_image(img, mask, use_rgb=True, alpha=0.5):\n",
        "    # Convert mask to a heatmap\n",
        "    heatmap = cv.applyColorMap(np.uint8(255 * mask), cv.COLORMAP_JET)\n",
        "    if use_rgb:\n",
        "        heatmap = cv.cvtColor(heatmap, cv.COLOR_BGR2RGB)\n",
        "        img = cv.cvtColor(np.uint8(255 * img), cv.COLOR_BGR2RGB)\n",
        "    else:\n",
        "        img = np.uint8(255 * img)\n",
        "    # Blend the heatmap with the original image\n",
        "    overlayed_img = cv.addWeighted(img, 1 - alpha, heatmap, alpha, 0)\n",
        "    return overlayed_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewlPTNrBJI2O"
      },
      "outputs": [],
      "source": [
        "#Creates the densenet model for the given number of classes\n",
        "def create_densenet121(num_classes):\n",
        "    # Load the DenseNet121 model without pre-trained weights\n",
        "    model = models.densenet121(weights=None)\n",
        "\n",
        "    # Modify the last fully connected layer to have the output features equal to the number of classes\n",
        "    num_ftrs = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "#Function used to create a Grad-Cam image for a specific disease\n",
        "def generate_gradcam(model, input_tensor, original_image, target_category=None):\n",
        "\n",
        "    #target_layers = model.features.denseblock4.denselayer16.conv2 #previously used layer\n",
        "    target_layers = model.features[-1]\n",
        "\n",
        "    # Initialize Grad-CAM with the specified target layers\n",
        "    cam = GradCAM(model=model, target_layers=[target_layers])  # Encapsulate target_layers in a list\n",
        "\n",
        "    # Define targets based on the specified target category\n",
        "    targets = [ClassifierOutputTarget(target_category)] if target_category is not None else None\n",
        "\n",
        "    # Generate CAM mask\n",
        "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0, :]\n",
        "\n",
        "    # Create visualization with the specified alpha for the overlay transparency\n",
        "    visualization = show_cam_on_image(np.array(original_image) / 255.0, grayscale_cam, use_rgb=True, alpha=0.3)\n",
        "\n",
        "    return visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgs2xNuOJNcV"
      },
      "outputs": [],
      "source": [
        "# Function to load checkpoint\n",
        "def load_checkpoint(model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    print(f\"Checkpoint loaded from '{checkpoint_path}'\")\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBkDbEzrJSpR"
      },
      "outputs": [],
      "source": [
        "# Function to save checkpoint\n",
        "def save_checkpoint(model, optimizer):\n",
        "    checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved to '{checkpoint_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iehI8AEDK0Gv"
      },
      "outputs": [],
      "source": [
        "#Server has of 4 GPU, manually set to avoid multiple models going to same GPU\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # Specify which GPU to use\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTmIq9y-Jswa"
      },
      "outputs": [],
      "source": [
        "# Model Training\n",
        "\n",
        "def train_model(train_loader, n_epochs, loadModel, learning_rate):\n",
        "\n",
        "    #creates an instance of the densenet 121 model for 6 classes\n",
        "    model = create_densenet121(6).to(device)\n",
        "\n",
        "    #defines optimizer using Adam optimizer\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Load checkpoint if specified\n",
        "    if loadModel:\n",
        "        model, opt = load_checkpoint(model, opt)\n",
        "        opt.param_groups[0]['lr'] = learning_rate\n",
        "\n",
        "    #loss function\n",
        "    #criterion = nn.CrossEntropyLoss()\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    losses = []\n",
        "    epochs = []\n",
        "\n",
        "    print(\"model parameters: \", next(model.parameters()).device)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f'Epoch {epoch}')\n",
        "        N = len(train_loader)\n",
        "        epoch_losses = []\n",
        "        for batch_index, (image, targets) in enumerate(train_loader):\n",
        "            image, targets = image.to(device), targets.to(device)\n",
        "            opt.zero_grad()\n",
        "            outputs = model(image)\n",
        "            loss_value = criterion(outputs, targets)\n",
        "\n",
        "            loss_value.backward()\n",
        "            opt.step()\n",
        "\n",
        "            epochs.append(epoch + batch_index / N)\n",
        "            losses.append(loss_value.item())\n",
        "            epoch_losses.append(loss_value.item())\n",
        "\n",
        "            # Release GPU memory\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        average_loss = np.mean(epoch_losses)\n",
        "        print(f'Average Loss for Epoch {epoch}: {average_loss}')\n",
        "\n",
        "        # Save checkpoint after each epoch\n",
        "        save_checkpoint(model, opt)\n",
        "\n",
        "    return model, np.array(epochs), np.array(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5ctUodSLHas"
      },
      "outputs": [],
      "source": [
        "#Computes performance metrics for each disease\n",
        "#Metrics include AUC, f1, precision, recall and prints ROC\n",
        "#It also produces the optimal thresholds\n",
        "\n",
        "def analyze_model(data_loader, model):\n",
        "    diseaseNames = [\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"No Finding\", \"Pleural Effusion\"]\n",
        "    bestThresholds = []\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predicted_probs.append(probabilities.cpu().numpy())\n",
        "            true_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    predicted_probs = np.concatenate(predicted_probs)\n",
        "    true_labels = np.concatenate(true_labels)\n",
        "\n",
        "    if np.any(np.isnan(true_labels)):\n",
        "        print(\"True labels contain NaN values.\")\n",
        "        true_labels[np.isnan(true_labels)] = 0\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    for i in range(len(diseaseNames)):\n",
        "        binary_true_labels = true_labels[:, i]\n",
        "        pred_probs_i = predicted_probs[:, i].flatten()\n",
        "\n",
        "        if len(pred_probs_i) != len(binary_true_labels):\n",
        "            raise ValueError(\"Mismatch in dimensions between predicted probabilities and true labels.\")\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(binary_true_labels, pred_probs_i)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        best_threshold_index = np.argmax(tpr - fpr)\n",
        "        best_threshold = thresholds[best_threshold_index]\n",
        "        bestThresholds.append(best_threshold)\n",
        "\n",
        "        binary_predictions = (pred_probs_i > best_threshold).astype(int)\n",
        "        precision = precision_score(binary_true_labels, binary_predictions)\n",
        "        recall = recall_score(binary_true_labels, binary_predictions)\n",
        "        f1 = f1_score(binary_true_labels, binary_predictions)\n",
        "\n",
        "        # Plot ROC curve for each disease\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'{diseaseNames[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "        print(f\"{diseaseNames[i]}:\")\n",
        "        print(f\"   AUC: {roc_auc:.4f}\")\n",
        "        print(f\"   Best Threshold: {best_threshold:.4f}\")\n",
        "        print(f\"   Precision: {precision:.4f}\")\n",
        "        print(f\"   Recall: {recall:.4f}\")\n",
        "        print(f\"   F1 Score: {f1:.4f}\\n\")\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    return bestThresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVtFtHJ6LJ1C"
      },
      "outputs": [],
      "source": [
        "#This block of code is used for training our model\n",
        "#The learnig rates were determined by running multiple instances (1-3)\n",
        "#of our model using different learning rates keeping whichever weights perfomed best\n",
        "\n",
        "# Start Model Training\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p10.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p10\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=False, learning_rate=0.001)\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p11.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p11\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.001)\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p12.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p12\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.00005)\n",
        "\n",
        "# copy_name = \"p10-p12_0.00005.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate/\" + copy_name)\n",
        "\n",
        "# file_path = \"/home/group18/test/done.txt\"\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "#     file.write(copy_name + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p13.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p13\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.00001)\n",
        "\n",
        "# copy_name = \"p10-p13_0.00001.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate/\" + copy_name)\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "# #     file.write(copy_name  + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p14.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p14\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.00001)\n",
        "\n",
        "# copy_name = \"p10-p14_0.00001.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate/\" + copy_name)\n",
        "\n",
        "# file_path = \"/home/group18/test/done.txt\"\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "#     file.write(copy_name + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p15.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p15\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.00001)\n",
        "\n",
        "# copy_name = \"p10-p15_0.00001.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate/\" + copy_name)\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "#     file.write(copy_name  + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p16.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p16\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.00001)\n",
        "\n",
        "# copy_name = \"p10-p16_0.00001.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate/\" + copy_name)\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "#     file.write(copy_name  + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Pass 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p10.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p10\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.00001)\n",
        "\n",
        "# copy_name = \"pass2_p10-p10_0.00001.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate/\" + copy_name)\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "#     file.write(copy_name  + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p11.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p11\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.0000005)\n",
        "\n",
        "# copy_name = \"pass_2_p10-p11_0.0000005.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate_2/\" + copy_name)\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "#     file.write(copy_name  + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p12.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p12\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.000005)\n",
        "\n",
        "# copy_name = \"pass_2_p10-p12_0.000005.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate_2/\" + copy_name)\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "#     file.write(copy_name  + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p13.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p13\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.000005)\n",
        "\n",
        "# copy_name = \"pass_2_p10-p13_0.000005.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate_2/\" + copy_name)\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "#     file.write(copy_name  + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p14.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p14\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.000005)\n",
        "\n",
        "# copy_name = \"pass_2_p10-p14_0.000005.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate_2/\" + copy_name)\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "#     file.write(copy_name  + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p15.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p15\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.000005)\n",
        "\n",
        "# copy_name = \"pass_2_p10-p15_0.000005.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate_2/\" + copy_name)\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "#     file.write(copy_name  + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# csv_file_path = \"/home/group18/Data/mimic/csv/train/p16.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/train/p16\"\n",
        "\n",
        "# train_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "# train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, pin_memory=True)\n",
        "\n",
        "# model, epoch_data, loss_data = train_model(train_loader, n_epochs=10, loadModel=True, learning_rate=0.000005)\n",
        "\n",
        "# copy_name = \"pass_2_p10-p16_0.000005.pth.tar\"\n",
        "# shutil.copy(checkpoint_path, \"/home/group18/Final/Saved_Models_Learning_Rate_2/\" + copy_name)\n",
        "\n",
        "# # Open the file in append mode\n",
        "# with open(\"/home/group18/Final/Model_Code/learning_finished.txt\", \"a\") as file:\n",
        "#     # Append text to the end of the file\n",
        "#     file.write(copy_name  + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx78KHNNKUHX"
      },
      "outputs": [],
      "source": [
        "# Load Validation Set\n",
        "\n",
        "# csv_file_path =  \"/home/group18/Data/mimic/csv/validation/p17.csv\"\n",
        "# root_dir_path = \"/home/group18/Data/mimic/validation/p17\"\n",
        "\n",
        "#Paths for Testing\n",
        "#Folders p18 and p18 must be combined before running\n",
        "csv_file_path =  \"/home/group18/Data/mimic/csv/test/p18_p19.csv\"\n",
        "root_dir_path = \"/home/group18/Data/mimic/test/p18_p19\"\n",
        "\n",
        "data_set = MIMIC_Dataset(csv_file=csv_file_path, root_dir=root_dir_path, transform=transforms.ToTensor())\n",
        "data_loader = DataLoader(dataset=data_set, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5nQ9HD-L1kQ"
      },
      "outputs": [],
      "source": [
        "# Run Model Validation/Testing\n",
        "\n",
        "# model = create_densenet121(6)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# loaded_model, loaded_optimizer = load_checkpoint(model, optimizer)\n",
        "# loaded_model.eval()\n",
        "# loaded_model.to(device)\n",
        "\n",
        "# analyze_model(data_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR8mKRvy-LXg"
      },
      "outputs": [],
      "source": [
        "#Runs our model on a single image and returns disease prediction, true labels and grad cam images\n",
        "\n",
        "\n",
        "def test_single_image(filepath, csv_file_path, thresholds, model, device):\n",
        "    # Read and preprocess the image\n",
        "    image = cv.imread(filepath)\n",
        "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "    original_image = Image.fromarray(image)\n",
        "    original_image = original_image.resize((256, 256))\n",
        "\n",
        "    # Convert the image to a PyTorch tensor and enable gradient\n",
        "    to_tensor = transforms.ToTensor()\n",
        "    image_tensor = to_tensor(original_image).unsqueeze(0).requires_grad_(True)\n",
        "    image_tensor = image_tensor.to(device)\n",
        "\n",
        "    # Ensures the model is on the correct device and set it to training mode temporarily\n",
        "    model = model.to(device)\n",
        "\n",
        "    prediction = []\n",
        "    true_labels = []\n",
        "    grad_cam_image = []\n",
        "\n",
        "    # Perform forward pass to get predictions and the last convolutional layer's activations\n",
        "    with torch.enable_grad():  # Ensure gradients are computed\n",
        "        outputs = model(image_tensor)\n",
        "        probability = torch.sigmoid(outputs).detach().cpu().numpy()\n",
        "\n",
        "        # Generate Grad-CAM for the highest scoring category\n",
        "        _, target_category = torch.max(outputs, 1)\n",
        "\n",
        "        for x in range (0,6):\n",
        "            grad_cam_image.append(generate_gradcam(model,image_tensor, original_image, target_category=x))\n",
        "\n",
        "    # Extract true labels from CSV file\n",
        "    filename_without_extension = os.path.splitext(os.path.basename(filepath))[0]\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "        datareader = csv.reader(f)\n",
        "        for row in datareader:\n",
        "            if row[0] == filename_without_extension:\n",
        "                true_labels.extend([int(float(row[i])) if row[i] == '1.0' else 0 for i in [12, 13, 14, 15, 20, 21]])\n",
        "                subject = row[1]\n",
        "                study = row[2]\n",
        "                break\n",
        "\n",
        "    # Compare predictions and true labels\n",
        "    for i in range(6):\n",
        "        prediction.append(1 if probability[0][i] >= thresholds[i] else 0)\n",
        "\n",
        "    print(\"probability: \", probability)\n",
        "    print(\"prediction: \", prediction)\n",
        "    print(\"true lables: \", true_labels)\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    print (\"subject id\", subject)\n",
        "    print(\"study id\", study)\n",
        "\n",
        "    # Create a table for visualization\n",
        "    array1 = ['Atelectasis', probability[0][0], prediction[0], true_labels[0]]\n",
        "    array2 = ['Cardiomegaly', probability[0][1], prediction[1], true_labels[1]]\n",
        "    array3 = ['Consolidation', probability[0][2], prediction[2], true_labels[2]]\n",
        "    array4 = ['Edema', probability[0][3], prediction[3], true_labels[3]]\n",
        "    array5 = ['No Finding', probability[0][4], prediction[4], true_labels[4]]\n",
        "    array6 = ['Pleural Effusion', probability[0][5], prediction[5], true_labels[5]]\n",
        "\n",
        "    table = [['Disease', 'Model Output', 'Model Prediction', 'True Labels'],\n",
        "             array1, array2, array3, array4, array5, array6]\n",
        "\n",
        "    # Print the table\n",
        "    print(\"\\n\")\n",
        "    print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
        "\n",
        "    return original_image, grad_cam_image, prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLX9n6ioouPx"
      },
      "outputs": [],
      "source": [
        "#Runs our model on a single image and returns disease prediction and grad cam images\n",
        "\n",
        "def test_single_image_no_csv(filepath, thresholds, model, device):\n",
        "    # Read and preprocess the image\n",
        "    image = cv.imread(filepath)\n",
        "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "    original_image = Image.fromarray(image)\n",
        "    original_image = original_image.resize((256, 256))\n",
        "\n",
        "    # Convert the image to a PyTorch tensor and enable gradient\n",
        "    to_tensor = transforms.ToTensor()\n",
        "    image_tensor = to_tensor(original_image).unsqueeze(0).requires_grad_(True)\n",
        "    image_tensor = image_tensor.to(device)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    prediction = []\n",
        "    grad_cam_image = []\n",
        "\n",
        "    # Perform forward pass to get predictions and the last convolutional layer's activations\n",
        "    with torch.enable_grad():  # Ensure gradients are computed\n",
        "        outputs = model(image_tensor)\n",
        "        probability = torch.sigmoid(outputs).detach().cpu().numpy()\n",
        "\n",
        "        # Generate Grad-CAM for the highest scoring category\n",
        "        _, target_category = torch.max(outputs, 1)\n",
        "\n",
        "        for x in range (0,6):\n",
        "            grad_cam_image.append(generate_gradcam(model,image_tensor, original_image, target_category=x))\n",
        "\n",
        "    # Compare predictions and true labels\n",
        "    for i in range(6):\n",
        "        prediction.append(1 if probability[0][i] >= thresholds[i] else 0)\n",
        "\n",
        "    print(\"probability: \", probability)\n",
        "    print(\"prediction: \", prediction)\n",
        "\n",
        "    # Create a table for visualization\n",
        "    array1 = ['Atelectasis', probability[0][0], prediction[0]]\n",
        "    array2 = ['Cardiomegaly', probability[0][1], prediction[1]]\n",
        "    array3 = ['Consolidation', probability[0][2], prediction[2]]\n",
        "    array4 = ['Edema', probability[0][3], prediction[3]]\n",
        "    array5 = ['No Finding', probability[0][4], prediction[4]]\n",
        "    array6 = ['Pleural Effusion', probability[0][5], prediction[5]]\n",
        "\n",
        "    table = [['Disease', 'Model Output', 'Model Prediction'],\n",
        "             array1, array2, array3, array4, array5, array6]\n",
        "\n",
        "    # Print the table\n",
        "    print(\"\\n\")\n",
        "    print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
        "\n",
        "    return original_image, grad_cam_image, prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEbbPtyI_Egc"
      },
      "outputs": [],
      "source": [
        "#Plots original and grad cam image side by side for easy comparison\n",
        "\n",
        "def plot_images(original_image, grad_cam_image, diseaseName):\n",
        "    # Create a figure with 2 subplots\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Plot the original image\n",
        "    axs[0].imshow(original_image)\n",
        "    axs[0].set_title('Original Image')\n",
        "    axs[0].axis('off')  # Hide the axes ticks\n",
        "\n",
        "    # Plot the Grad-CAM heatmap\n",
        "    axs[1].imshow(grad_cam_image)\n",
        "    axs[1].set_title('Grad-CAM Heatmap for ' + diseaseName)\n",
        "    axs[1].axis('off')  # Hide the axes ticks\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NAOf2rtouPx"
      },
      "outputs": [],
      "source": [
        "#These functions are shared between backend and frontend and are used to convert images to byte array so they can be passed in a json\n",
        "\n",
        "\n",
        "#Converts JPG image to RGB and resizes\n",
        "def jpg256(jpg_path):\n",
        "        image = skio.imread(jpg_path)\n",
        "        RGB = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "        new_image = Image.fromarray(RGB)\n",
        "        new_image = new_image.resize((256, 256))\n",
        "        return new_image\n",
        "\n",
        "#Converts jpg images to tensor\n",
        "def Tensor256(jpg_path):\n",
        "        image = skio.imread(jpg_path)\n",
        "        RGB = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "        new_image = Image.fromarray(RGB)\n",
        "        new_image = new_image.resize((256, 256))\n",
        "\n",
        "        to_tensor = transforms.ToTensor()\n",
        "        image_tensor = to_tensor(new_image)\n",
        "        return image_tensor\n",
        "\n",
        "\n",
        "#Converts an image to RGB and resizes then converts to tensor\n",
        "def Grad_Tensor256(jpg_path,device):\n",
        "        image = skio.imread(jpg_path)\n",
        "        RGB = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "        new_image = Image.fromarray(RGB)\n",
        "        new_image = new_image.resize((256, 256))\n",
        "\n",
        "        to_tensor = transforms.ToTensor()\n",
        "        new_image =  to_tensor(new_image).unsqueeze(0).requires_grad_(True)\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        return to_tensor\n",
        "\n",
        "\n",
        "#converts JPEG to a byte representation\n",
        "def image_to_base64(image_array):\n",
        "        image = Image.fromarray(np.uint8(image_array)).convert('RGB')\n",
        "        img_byte_arr = io.BytesIO()\n",
        "        image.save(img_byte_arr, format='JPEG')\n",
        "        return base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')\n",
        "\n",
        "\n",
        "# Function to preprocess and convert image to byte array\n",
        "def preprocess_and_convert_to_byte_array(image_path):\n",
        "        # Preprocess the image\n",
        "        image = skio.imread(image_path)\n",
        "        RGB = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "        new_image = Image.fromarray(RGB)\n",
        "        new_image = new_image.resize((256, 256))\n",
        "\n",
        "        # Convert to byte array\n",
        "        img_byte_arr = io.BytesIO()\n",
        "        new_image.save(img_byte_arr, format='JPEG')\n",
        "        img_byte_arr = img_byte_arr.getvalue()\n",
        "        return img_byte_arr\n",
        "\n",
        "# Function to convert byte array back to image\n",
        "def byte_array_to_image(byte_array):\n",
        "        img_byte_arr = io.BytesIO(byte_array)\n",
        "        img = Image.open(img_byte_arr)\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFjrPz2SouPx"
      },
      "outputs": [],
      "source": [
        "#Runs our model on a single image and returns disease prediction, true labels and grad cam images\n",
        "#Then prints and plots results\n",
        "\n",
        "def run_with_csv(filepath, csv_filepath):\n",
        "\n",
        "    model = create_densenet121(6)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
        "    loaded_model, loaded_optimizer = load_checkpoint(model, optimizer)\n",
        "    loaded_model.eval()\n",
        "    loaded_model.to(device)\n",
        "\n",
        "    #thresholds are old and determine if model predictions are 1 or 0 based on model probability\n",
        "    thresholds = [0.17107886, 0.15799165, 0.03852078, 0.09797211, 0.3499906, 0.1622753]\n",
        "\n",
        "    diseaseNames = [\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"No Finding\", \"Pleural Effusion\"]\n",
        "\n",
        "    print(\"thresholds: \", thresholds)\n",
        "\n",
        "    original_image, grad_cam_image, predictions = test_single_image(filepath, csv_filepath, thresholds, loaded_model, device)\n",
        "\n",
        "    for x in range(0,6):\n",
        "        plot_images(original_image, grad_cam_image[x], diseaseNames[x])\n",
        "\n",
        "    grad_cam_images_base64 = [image_to_base64(img) for img in grad_cam_image]\n",
        "\n",
        "    diseases_data = []\n",
        "    for i, disease_name in enumerate(diseaseNames):\n",
        "        diseases_data.append({\n",
        "            \"diseaseName\": disease_name,\n",
        "            \"prediction\": predictions[i],\n",
        "            \"gradCamImage\": grad_cam_images_base64[i]\n",
        "        })\n",
        "\n",
        "    data = { \"diseasesData\": diseases_data }\n",
        "\n",
        "    # Convert the structured data to a JSON string\n",
        "    json_data = json.dumps(data, indent=4)\n",
        "\n",
        "    return json_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCJdVskjouPx"
      },
      "outputs": [],
      "source": [
        "#Runs our model on a single image and returns disease prediction and grad cam images\n",
        "#Then prints and plots results\n",
        "\n",
        "def run_with_no_csv(filepath):\n",
        "\n",
        "    model = create_densenet121(6)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
        "    loaded_model, loaded_optimizer = load_checkpoint(model, optimizer)\n",
        "    loaded_model.eval()\n",
        "    loaded_model.to(device)\n",
        "\n",
        "    #thresholds are old and determine if model predictions are 1 or 0 based on model probability\n",
        "    thresholds = [0.17107886, 0.15799165, 0.03852078, 0.09797211, 0.3499906, 0.1622753]\n",
        "\n",
        "    diseaseNames = [\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"No Finding\", \"Pleural Effusion\"]\n",
        "\n",
        "    print(\"thresholds: \", thresholds)\n",
        "\n",
        "    original_image, grad_cam_image, predictions = test_single_image_no_csv(filepath, thresholds, loaded_model, device)\n",
        "\n",
        "    for x in range(0,6):\n",
        "        plot_images(original_image, grad_cam_image[x], diseaseNames[x])\n",
        "\n",
        "\n",
        "    grad_cam_images_base64 = [image_to_base64(img) for img in grad_cam_image]\n",
        "\n",
        "    diseases_data = []\n",
        "    for i, disease_name in enumerate(diseaseNames):\n",
        "        diseases_data.append({\n",
        "            \"diseaseName\": disease_name,\n",
        "            \"prediction\": predictions[i],\n",
        "            \"gradCamImage\": grad_cam_images_base64[i]\n",
        "        })\n",
        "\n",
        "    data = { \"diseasesData\": diseases_data }\n",
        "\n",
        "    # Convert the structured data to a JSON string\n",
        "    json_data = json.dumps(data, indent=4)\n",
        "\n",
        "    return json_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fVbK7muouPx"
      },
      "outputs": [],
      "source": [
        "#This block is for manually inspection of GradGam outputs\n",
        "#The results of manual expection can be found on Git page called Group_18_Testing_Document.pdf under docs\n",
        "\n",
        "\n",
        "model = create_densenet121(6)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loaded_model, loaded_optimizer = load_checkpoint(model, optimizer)\n",
        "loaded_model.eval()\n",
        "loaded_model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Atelectasis Testing Image 1\n",
        "# p18_p19/p19/p19109135/s51197801.txt\n",
        "#img = \"20d9383c-3fa80c3c-94218c7f-15020bd1-e47ed769\"\n",
        "#img = \"ae4d45fc-815b6cd8-d29c078f-ad849410-cbb8cf47\"\n",
        "\n",
        "\n",
        "# Atelectasis Testing Image 2\n",
        "# p18_p19/p19/p19173988/s54718065.txt\n",
        "#img = \"52b360b9-9fe4bb09-98bffb7f-3d8e4391-863e32e3\"\n",
        "#img = \"f556ae7b-066cd0b1-954eaf59-4c30d965-f86260ff\"\n",
        "\n",
        "# Atelectasis Testing Image 3\n",
        "# p18_p19/p19/p19806884/s59414102.txt\n",
        "#img = \"d944bdaf-e3e6f59b-f08a84f6-86f07ba4-6c783f84\"\n",
        "\n",
        "\n",
        "# Atelectasis Testing Image 4\n",
        "# p18_p19/p18/p18741255/s51561605.txt\n",
        "#img = \"34589968-8b864391-ae8fda46-91ab2791-4daf87af\"\n",
        "\n",
        "\n",
        "# Atelectasis Testing Image 5\n",
        "# p18_p19/p19/p19229277/s53608284.txt\n",
        "#img = \"cf875bfb-1ea9aff7-cb1c26b0-e1598bfb-b193ff62\"\n",
        "\n",
        "\n",
        "\n",
        "# Cardiomegaly Testing Image 1\n",
        "# p18_p19/p18/p18396526/s52262111.txt\n",
        "#img = \"3f2023b8-39fdbbf0-56b564b1-725766fd-4521856f\"\n",
        "\n",
        "# Cardiomegaly Testing Image 2\n",
        "# p18_p19/p18/p18712968/s58288058.txt\n",
        "#img = \"01a9c196-4d0a3697-86bc7015-19c697c3-8a6fd8ea\"\n",
        "#img = \"e29e98e9-2c21732f-ce097e4f-90427d27-6afd093c\"\n",
        "\n",
        "# Cardiomegaly Testing Image 3\n",
        "# p18_p19/p19/p19807980/s57843918.txt\n",
        "#img = \"379d3644-53dcc490-f52c384f-4905bd21-9813c730\"\n",
        "#img = \"2886d54b-ffa8bbe0-506cdef6-b8dd2ae4-c533ed1c\"\n",
        "\n",
        "# Cardiomegaly Testing Image 4\n",
        "# p18_p19/p19/p19817306/s53766057.txt\n",
        "#img = \"e724f8b3-d68a8347-10f8d635-445486dd-5e1f1517\"\n",
        "\n",
        "# Cardiomegaly Testing Image 5\n",
        "# p18_p19/p19/p19840732/s58791803.txt\n",
        "#img = \"a424fc43-96f3e8ef-34323d26-38a8ad6b-13998689\"\n",
        "\n",
        "\n",
        "# Consolidation Testing Image 1\n",
        "#  p18_p19/p18/p18722792/s54662820.txt\n",
        "#img = \"1b854fc7-523ffa19-f61d6f35-a08a3097-a6286649\"\n",
        "\n",
        "# Consolidation Testing Image 2\n",
        "# p18_p19/p18/p18716038/s52770632.txt\n",
        "#img = \"d5f42cd8-b55716cb-6df1e516-7e45a869-8d6cca95\"\n",
        "#img = \"f97eed61-812d052b-6bf05c8b-072a8076-c83d9d3f\"\n",
        "\n",
        "# Consolidation Testing Image 3\n",
        "# p18_p19/p19/p19774163/s58753570.txt\n",
        "#img = \"7474687d-324ff331-2e95f8d3-b7914d7c-c3d1de52\"\n",
        "#img = \"a2212511-518016f2-5565443e-14f03252-12c867d1\"\n",
        "\n",
        "# Consolidation Testing Image 4\n",
        "# p18_p19/p18/p18821140/s58495446.txt\n",
        "#img = \"ebc940d3-b17d2c78-26fe5f74-2031a732-6e1ccd22\"\n",
        "\n",
        "# Consolidation Testing Image 5\n",
        "# p18_p19/p19/p19723160/s57120919.txt\n",
        "#img = \"7b385b7f-04ab764c-05c36ce0-5d9ed945-f3476595\"\n",
        "#img = \"b3d4be3e-4e06b970-b7b96c8e-c431edfb-19aa5f41\"\n",
        "\n",
        "\n",
        "\n",
        "# Edema Testing Image 1\n",
        "# p18_p19/p18/p18017335/s55311626.txt\n",
        "#img = \"dab94a45-c2c94579-3607bbaf-575b5829-26801e35\"\n",
        "\n",
        "# Edema Testing Image 2\n",
        "# p18_p19/p18/p18031120/s55643091.txt\n",
        "#img = \"0a23b5f3-77bca5ef-86f4365e-cae269b8-aa497f1a\"\n",
        "\n",
        "\n",
        "# Edema Testing Image 3\n",
        "# p18_p19/p18/p18033645/s55530651.txt\n",
        "#img = \"31faa2b6-3a8899c4-521ceae0-cefdb8f4-3b2f3680\"\n",
        "\n",
        "\n",
        "# Edema Testing Image 4\n",
        "# p18_p19/p18/p18063505/s56916046.txt\n",
        "#img = \"ab47c784-86aedf07-753e13a9-ce23e2c6-e997843b\"\n",
        "#img = \"28a689df-ddc46bd8-35daa760-5a5c309d-06758a2c\"\n",
        "\n",
        "# Edema Testing Image 5\n",
        "# p18_p19/p18/p18108905/s50619037.txt\n",
        "#img = \"eeb6830f-5ab8e567-857fd6ed-5cec9b4a-177c157f\"\n",
        "\n",
        "\n",
        "\n",
        "# No Finding Testing Image 1\n",
        "# p18_p19/p18/p18000291/s55388853.txt\n",
        "#img = \"04564240-d4e9e69c-1dd70a83-14b463cd-b7614743\"\n",
        "#img = \"a17c02b3-7c360f2c-a07734a0-76867697-36d59d3d\"\n",
        "#img = \"ed82a88d-499bed8a-de7539b2-417328ad-8aac20ee\"\n",
        "\n",
        "# No Finding Testing Image 2\n",
        "# p18_p19/p18/p18000379/s55947854.txt\n",
        "#img = \"6a9de243-1caf11ca-7f966bc6-0201c9e8-0dea45a3\"\n",
        "#img = \"964333fa-fd1c609d-9161adb3-03ef590f-45a8a95c\"\n",
        "\n",
        "# No Finding Testing Image 3\n",
        "# p18_p19/p18/p18000735/s50785186.txt\n",
        "#img = \"e37d4ad7-dd9760f8-a2435064-618c3875-b85fad9c\"\n",
        "#img = \"f0d4b86f-aface71f-579cb776-b40850e6-948c4b8f\"\n",
        "\n",
        "# No Finding Testing Image 4\n",
        "# p18_p19/p18/p18000818/s59355587.txt\n",
        "#img = \"1cf26fdd-54faeea9-6e331d75-f17c4cb9-11052ec0\"\n",
        "#img = \"4ed3eece-cf05be86-c4f43e0d-6fe4a79b-acad7a8c\"\n",
        "\n",
        "# No Finding Testing Image 5\n",
        "# p18_p19/p18/p18001129/s51043598.txt\n",
        "#img = \"11551ba5-8db1caaa-01677a1a-4b8297ef-10062dc7\"\n",
        "#img = \"973cc278-9b901ebc-9e0223d1-bee44581-e8b6452f\"\n",
        "\n",
        "\n",
        "\n",
        "# Pleural Effusion Testing Image 1\n",
        "# p18_p19/p18/p18003894/s56902065.txt\n",
        "#img = \"98b2b140-a031b7d8-a5da775a-d9d0fb3d-79dfdc83\"\n",
        "\n",
        "# Pleural Effusion Testing Image 2\n",
        "# p18_p19/p18/p18019452/s57426164.txt\n",
        "#img = \"363ec58a-5b27a2a2-d43ad3f9-f9f36538-f898b625\"\n",
        "\n",
        "# Pleural Effusion Testing Image 3\n",
        "# p18_p19/p18/p18033645/s55530651.txt\n",
        "#img = \"31faa2b6-3a8899c4-521ceae0-cefdb8f4-3b2f3680\"\n",
        "\n",
        "# Pleural Effusion Testing Image 4\n",
        "# p18_p19/p18/p18042178/s52595621.txt\n",
        "#img = \"9df9e68a-f1169710-43c90f3c-402cc9f6-39d6b824\"\n",
        "\n",
        "# Pleural Effusion Testing Image 5\n",
        "# p18_p19/p18/p18042178/s55414081.txt\n",
        "img = \"a73f67cb-036cb1ae-34b24fc2-725883fd-b5961466\"\n",
        "\n",
        "\n",
        "filepath = \"/home/group18/Data/mimic/test/p18_p19/\" + img + \".jpg\"\n",
        "csv_file_path = \"/home/group18/Data/mimic/csv/test/p18_p19.csv\"\n",
        "\n",
        "thresholds = [0.17107886, 0.15799165, 0.03852078, 0.09797211, 0.3499906, 0.1622753]\n",
        "diseaseNames = [\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"No Finding\", \"Pleural Effusion\"]\n",
        "\n",
        "print(\"thresholds: \", thresholds)\n",
        "\n",
        "\n",
        "original_image, grad_cam_image,_ = test_single_image(filepath, csv_file_path, thresholds, model, 'cuda')\n",
        "\n",
        "for x in range(0,6):\n",
        "    plot_images(original_image, grad_cam_image[x], diseaseNames[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQeZNda8ouPy"
      },
      "outputs": [],
      "source": [
        "#This block is used to test the average run time of the model\n",
        "\n",
        "# total_time = 0\n",
        "\n",
        "\n",
        "# img = \"/home/group18/Data/mimic/test/p18_p19/a73f67cb-036cb1ae-34b24fc2-725883fd-b5961466.jpg\"\n",
        "\n",
        "# #Before testing comment out print statements in run_with_no_csv and test_single_image_no_csv and load_checkpoint\n",
        "# for x in range (0,20):\n",
        "#     start_time = time.time()\n",
        "#     run_with_no_csv(img)\n",
        "#     end_time = time.time()\n",
        "#     total_time += end_time - start_time\n",
        "\n",
        "# print(\"Average Time in Seconds\", total_time / 20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}